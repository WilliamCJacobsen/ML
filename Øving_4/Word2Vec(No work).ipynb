{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dette er markdown.\n",
    "## Her kan jeg sette in SUBTITLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ðŸ’— python\n"
     ]
    }
   ],
   "source": [
    "this_is_python = \"I ðŸ’— python\"\n",
    "print(this_is_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onehot encoding of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = \" hello world\"\n",
    "y_data = \"hello world \"\n",
    "int_char = dict((i, c) for (i,c) in enumerate(\"\".join(set(x_data))))\n",
    "dictonary = dict((c, i) for (i,c) in enumerate(\"\".join(set(x_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [list(keras.utils.to_categorical(list(dictonary[val] for val in x_data)))]\n",
    "y_train = [list(keras.utils.to_categorical(list(dictonary[val] for val in y_data)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12, 8)\n",
      "[0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(x_train).shape)\n",
    "print(x_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_in_LSTM = 128\n",
    "batch_size = 8\n",
    "epochs = 1000\n",
    "\n",
    "x = tf.placeholder(shape = [None, None, batch_size], dtype = tf.float32, name = \"x\")\n",
    "y_model = tf.placeholder(shape = [None, None, batch_size], dtype = tf.float32, name = \"y\")\n",
    "state_value = tf.placeholder(dtype = tf.int32, shape = [])\n",
    "\n",
    "w = tf.Variable(tf.random_uniform([units_in_LSTM, batch_size]),shape=[units_in_LSTM,batch_size], dtype = tf.float32, name = \"weight\")\n",
    "b = tf.Variable(tf.random_uniform([batch_size]), dtype = tf.float32, shape=[batch_size], name = \"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0917 13:12:00.439109 4512822720 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0917 13:12:00.440053 4512822720 deprecation.py:323] From <ipython-input-7-ff1732769819>:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(units_in_LSTM)\n",
    "in_state = cell.zero_state(state_value, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 13:12:00.467050 4512822720 deprecation.py:323] From <ipython-input-8-a5f8a652e51d>:1: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0917 13:12:00.519475 4512822720 deprecation.py:506] From /Users/williamjacobsen/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0917 13:12:00.526454 4512822720 deprecation.py:506] From /Users/williamjacobsen/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0917 13:12:00.911621 4512822720 deprecation.py:323] From /Users/williamjacobsen/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0917 13:12:01.121424 4512822720 deprecation.py:506] From /Users/williamjacobsen/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "lstm, out_state = tf.nn.dynamic_rnn(cell, x, initial_state=in_state)\n",
    "\n",
    "logits = tf.nn.bias_add(tf.einsum('bts,se->bte', lstm, w), b) \n",
    "softie = tf.nn.softmax(logits)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(softie, logits)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate = 0.09)\n",
    "trainer = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss 2.052191\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 10\n",
      "loss 2.052191\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 20\n",
      "loss 2.052191\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 30\n",
      "loss 2.052191\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 40\n",
      "loss 2.052191\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 50\n",
      "loss 2.052191\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 60\n",
      "loss 2.052191\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 70\n",
      "loss 2.052191\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 80\n",
      "loss 2.052191\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 90\n",
      "loss 2.0521908\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 100\n",
      "loss 2.0521908\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 110\n",
      "loss 2.0521908\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 120\n",
      "loss 2.052191\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 130\n",
      "loss 2.052191\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 140\n",
      "loss 2.0521917\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 150\n",
      "loss 2.0521924\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 160\n",
      "loss 2.052193\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 170\n",
      "loss 2.0521982\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 180\n",
      "loss 2.0522065\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 190\n",
      "loss 2.0522091\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 200\n",
      "loss 2.0522096\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 210\n",
      "loss 2.052224\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 220\n",
      "loss 2.052212\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 230\n",
      "loss 2.0521357\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 240\n",
      "loss 2.0521219\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 250\n",
      "loss 2.0521338\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 260\n",
      "loss 2.052192\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 270\n",
      "loss 2.0521963\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 280\n",
      "loss 2.0521278\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 290\n",
      "loss 2.052036\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 300\n",
      "loss 2.0520108\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 310\n",
      "loss 2.051983\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 320\n",
      "loss 2.0520184\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 330\n",
      "loss 2.0520296\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 340\n",
      "loss 2.0521288\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 350\n",
      "loss 2.0520697\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 360\n",
      "loss 2.052093\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 370\n",
      "loss 2.0520203\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 380\n",
      "loss 2.0520327\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 390\n",
      "loss 2.0519285\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 400\n",
      "loss 2.0519314\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 410\n",
      "loss 2.0519617\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 420\n",
      "loss 2.0519938\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 430\n",
      "loss 2.0519786\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 440\n",
      "loss 2.0519352\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 450\n",
      "loss 2.0519273\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 460\n",
      "loss 2.0519733\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 470\n",
      "loss 2.0519867\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 480\n",
      "loss 2.0520828\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "epoch 490\n",
      "loss 2.0521908\n",
      " hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    zero_state = sess.run(in_state, {state_value: 1})\n",
    "\n",
    "    for i in range(500):\n",
    "        feed = {state_value: 1, x: x_train, y_model: y_train, in_state : zero_state}\n",
    "        sess.run(trainer, feed)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"epoch\", i)\n",
    "            print(\"loss\", sess.run(loss, {state_value: 1, x: x_train, y_model: y_train, in_state: zero_state}))\n",
    "            # Generate characters from the initial characters ' h'\n",
    "            state = sess.run(in_state, {state_value: 1})\n",
    "            text = ' h'\n",
    "            y, state = sess.run([softie, out_state], {state_value: 1, x: [[x_train[0][0]]], in_state: state})  # ' '\n",
    "            y, state = sess.run([softie, out_state], {state_value: 1, x: [[x_train[0][1]]], in_state: state})  # 'h'\n",
    "        \n",
    "            text += int_char[y.argmax()]\n",
    "            for c in range(50):\n",
    "                y, state = sess.run([softie, out_state], {state_value: 1, x: [[x_train[0][y.argmax()]]], in_state: state})\n",
    "                text += int_char[y[0].argmax()]\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
