{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = \" hello world h\"\n",
    "y_data = \"hello world he\"\n",
    "\n",
    "int_char = dict((i, c) for (i,c) in enumerate(\"\".join(set(x_data))))\n",
    "dictonary = dict((c, i) for (i,c) in enumerate(\"\".join(set(x_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 14, 8)\n"
     ]
    }
   ],
   "source": [
    "x_train = [[list(keras.utils.to_categorical(list(dictonary[val] for val in x_data)))]]\n",
    "y_train = [[list(keras.utils.to_categorical(list(dictonary[val] for val in y_data)))]]\n",
    "(_,_,_,batch_size) = np.array(x_train).shape\n",
    "print(np.array(x_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_in_LSTM = 128\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0917 13:08:05.644299 4376851904 deprecation.py:506] From /Users/williamjacobsen/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.LSTM(units_in_LSTM, input_shape=(None, batch_size), return_sequences=True),\n",
    "    keras.layers.Dense(batch_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, data):\n",
    "    if epoch % 10 == 9:\n",
    "        print(\"epoch\", epoch)\n",
    "        print(\"loss\", data['loss'])\n",
    "        \n",
    "        # Generate text from the initial text ' h'\n",
    "        text = ' h'\n",
    "        for i in range(50):\n",
    "            x = [[list(keras.utils.to_categorical(list(dictonary[val] for val in list(text)), batch_size))]]\n",
    "            y = model.predict(x)[0][-1]\n",
    "            text += int_char[y.argmax()]\n",
    "        print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 13:08:08.455049 4376851904 deprecation.py:323] From /Users/williamjacobsen/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0917 13:08:08.950074 4376851904 deprecation.py:506] From /Users/williamjacobsen/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 14, 8)\n",
      "epoch 9\n",
      "loss 1.9506261348724365\n",
      " hllllllll llll llll llll llll llll llll llll llll l\n",
      "(1, 1, 14, 8)\n",
      "epoch 19\n",
      "loss 1.749611258506775\n",
      " hlllo  lo l hell o l hell o hell l o hell l o hell \n",
      "(1, 1, 14, 8)\n",
      "epoch 29\n",
      "loss 1.0686455965042114\n",
      " helloo r heelll heell  hell l heell l heell  hell l\n",
      "(1, 1, 14, 8)\n",
      "epoch 39\n",
      "loss 1.0306427478790283\n",
      " helooo wrd helo old helo old helo old helo old helo\n",
      "(1, 1, 14, 8)\n",
      "epoch 49\n",
      "loss 0.242585688829422\n",
      " helo world hello world hello world hello world hell\n",
      "(1, 1, 14, 8)\n",
      "epoch 59\n",
      "loss 0.014913973398506641\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 69\n",
      "loss 0.0056079076603055\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 79\n",
      "loss 0.0024329866282641888\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 89\n",
      "loss 0.0011317504104226828\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 99\n",
      "loss 0.000549967575352639\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 109\n",
      "loss 0.00027648467221297324\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 119\n",
      "loss 0.00014352230937220156\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 129\n",
      "loss 7.536607154179364e-05\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 139\n",
      "loss 4.002585410489701e-05\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 149\n",
      "loss 2.15389682125533e-05\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 159\n",
      "loss 1.1712436389643699e-05\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 169\n",
      "loss 6.441597633966012e-06\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 179\n",
      "loss 3.559260903784889e-06\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 189\n",
      "loss 1.9754720597120468e-06\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 199\n",
      "loss 1.1154596677442896e-06\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 209\n",
      "loss 6.343640848172072e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 219\n",
      "loss 4.0020273672780604e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 229\n",
      "loss 2.639634999468399e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 239\n",
      "loss 1.9584386734550208e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 249\n",
      "loss 1.7029900334364356e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 259\n",
      "loss 1.617840297285511e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 269\n",
      "loss 1.4475413934178505e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 279\n",
      "loss 1.3623919414840202e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 289\n",
      "loss 1.319817073408558e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 299\n",
      "loss 1.319817073408558e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 309\n",
      "loss 1.2772423474416428e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 319\n",
      "loss 1.2346676214747276e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 329\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 339\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 349\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 359\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 369\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 379\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 389\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 399\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 409\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 419\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 429\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 439\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 449\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 459\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 469\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 479\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 489\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n",
      "(1, 1, 14, 8)\n",
      "epoch 499\n",
      "loss 1.1920928955078125e-07\n",
      " hello world hello world hello world hello world hel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63a71cef0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=500, batch_size=1,verbose = False, callbacks=[tf.keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
